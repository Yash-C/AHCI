<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link href="https://fonts.googleapis.com/css?family=Montserrat&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
    <script src="https://kit.fontawesome.com/d61fe607bc.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="style/style.css">
    <title>AHCI</title>
</head>

<body>
    <header>
        <!-- Navbar below -->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary">
            <a class="navbar-brand" href="index.html"><i class="fas fa-laptop-code"></i>AHCI</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarText"
                aria-controls="navbarText" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarText">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item ">
                        <a class="nav-link" href="index.html">Home</a>
                    </li>
                    <li class="nav-item ">
                        <a class="nav-link" href="methodology.html">Methodology</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="analysis.html">Analysis</a>
                    </li>
                    <li class="nav-item px--1">
                        <a class="nav-link" href="results.html">Results</a>
                    </li>
                    <li class="nav-item dropdown px-2">
                        <a class="nav-link dropdown-toggle" href="#" id="navbardrop" data-toggle="dropdown">
                          Reflections
                        </a>
                        <div class="dropdown-menu">
                          <a class="dropdown-item" href="yash.html">Yash Chaudhari</a>
                          <a class="dropdown-item" href="rachel.html">Rachel Lee</a>
                          <a class="dropdown-item" href="matt.html">Matt Ripia</a>
                          <a class="dropdown-item" href="jamie.html">Jaime King</a>
                        </div>
                      </li>
                </ul>
            </div>
        </nav>
    </header>

    <div class="body">
        <div class="jumbotron feature">
            <div class="container">
                <h1><strong>Usability Research</strong></h1>
            </div>
        </div>
        <!-- Content -->
        <div class="container">
            <!-- Page Intro -->
            <div class="row page-intro">
                <div class="introduction_paragraph">

                    <h2>Quantitative vs Qualitative vs Lab Experiment</h2>
                    <h3>Quantitative:</h3>
                    <p>Quantitative user testing is mostly used at the start (evaluation of the initial design) or at
                        the end (evaluation of the redesign) of the design cycle, when the functioning product is ready.
                        In these stages, sometimes multiple quantitative researches are conducted in order to see the
                        clear comparison of the different version of the product. Through this benchmarking, testers are
                        able to detect the level of improvement in the design with data analysis (Budiu, 2017).</p>
                    <p>In this testing, testers are to collect measurable numeric data that can be used in statistical
                        analysis from a large pool of participants. This data is collected by recording screen
                        movements, mouse clicks or time taken to complete tasks while observing the participants. The
                        improvement of the new version can be measured by comparing the time and other numerical data
                        and seeing how these differentiate between versions. (Gleason, 2019).</p>
                    <h3>Qualitative: </h3>
                    <p>Qualitative user testing can be used at any phase of the design cycle. It is a cost and time
                        efficient way of gathering data compared to quantitative studies and for this reason it is
                        considered to be more suitable to use in the redesigning stage, where the testers are trying to
                        identify any design flaws in the product (Budiu, 2017). </p>
                    <p>In this testing, testers collect non-numerical data such as findings and the result of informal
                        interviews that can uncover design flaws or strengths from a small pool of users. Users are
                        given tasks to complete and are instructed to “think out loud” while navigating the site, and
                        the researchers are to observe and record the narration. Through this exercise, the researchers
                        are able to gain insight to what the users feel about each features of the product (Gleason,
                        2019).</p>
                    <p>Budiu (2017) states that there is a high chance of identifying 85% of the design flaws with only
                        5 subjects, given the product is not in the final stage. Therefore, it is efficient to repeat
                        this process to discover any major problems that need to be fixed, with each version in the
                        redesigning phase (Budiu, 2017). </p>
                    <h3>Lab Experiment:</h3>
                    <p>Lab experiments are conducted to prove theories and test hypothesis so the result of the
                        experiment will either validate or reject the proposed hypothesis (Dumas & Redish, 1999).
                        They are only ever conducted in labs and the scope of the lab experiments are exceptionally
                        controlled as it is a good practice to test with a small set of variables (Seffah & Metzker,
                        2009).
                    </p>
                    <p>With experiments, the researchers are able to isolate specific variables with high level of
                        control, and measure the relationships among them. They can also be duplicated if further study
                        is required, providing advanced conclusions. However, they are costly and require larger amount
                        of subjects to draw more accurate results. Also, the experiments often do not represent the
                        real-world, as the variables are tightly-controlled (Basili, 2007).</p>
                    <h3>Chosen Usability Test</h3>
                    <p>After some initial research it was clear that we needed to conduct a qualitative usability study,
                        which aligns with the purpose of the study.
                        By utilizing a qualitative study, we are able to have participants use the application to
                        complete a task and watch them while they navigate the site, then complete an interview to get
                        their thoughts. We chose to ask 10 participants to be involved in this process as we feel like
                        anymore than this will give diminishing returns in terms of usable feedback.
                    </p>
                    <p>We chose this technique because it allows us to gain additional data other than just numbers and
                        metrics which will allow our team to identify common usability issues as well as finding
                        improvements to these. This type of study is also quite flexible and somewhat informal.
                        Participants are able to talk aloud as they are using the application and while we are watching
                        them, we can pick up on cues that show their confusion, (such as the look on their face as they
                        found something were unsure about or how frustrated they were getting while using the
                        application and didn't know what to do.) From here we can ask them about those areas and get
                        additional details about why they got stuck or what they were having trouble with during a post
                        interview.</p>
                    <h3>Method</h3>
                    <p>From our research about the different types of studies we could conduct to check the usability of
                        an application, we ended up choosing a qualitative usability study which consisted of the
                        following steps.
                    </p>
                    <p>1: We first set up a scenario that we wanted the participants to walk-through. This was to ensure
                        that all of the functionality of the application is or can be used. We did this to get the most
                        coverage of the application and to see if participants would actually be using the functionality
                        or if it was easily missed or misused.</p>
                    <img src="images/scenario.png" alt="Scenario">
                    <p>This scenario consisted of them choosing 3 papers that AUT offers from semester 2 of 2020. (Which
                        ensures they use the selection filters as well as using the paper search to find specific
                        papers). Participants had 5 minutes to find an ideal timetable before they were asked to stop.
                        We chose 5 minutes because the team felt this was plenty of time to achieve the task and if
                        participants went over this time, it showed some major usability issues.</p>
                    <img src="images/paper selection.jpg" alt="paper selector image">
                    <p>From here they were required to search through the list of generated timetables and find one with
                        a specific criteria, in this case they were looking for a tuesday off. This ensured they used
                        the ‘slider’ which is a horizontal scroll bar to scroll through the generated timetables as well
                        as the filters which sorted the results based on criteria (like days off). </p>
                    <img src="images/timetable.png" alt="timetables image">
                    <p>2: We then created a set of post interview questions which we used on the completion of their
                        task (or if they ran out of time). This consisted of 10 questions that were generally very open
                        as we wanted to get their opinions without influencing their answers. This consisted of the
                        following:</p>
                    <li>What was the most difficult part of the website and what made you struggle in it?</li>
                    <li>What were your initial thoughts of the website did it look good, bad how (initial perception)
                    </li>
                    <li>Which part did you get stuck at?</li>
                    <li>What was your favorite feature of the website</li>
                    <li>What would you like to see added to make it easier?</li>
                    <li>Did you feel you needed more time?</li>
                    <li>If you had another go would it be easier for you now that you have used it once?</li>
                    <li>Do you think you used all the features that the website had to offer?</li>
                    <li>Did any parts of the website not work?</li>
                    <li>List of all the issues that you noticed.</li>
                    <br>
                    <p>We chose these questions as a way to gather as many usability flaws that we could. We were also
                        watching the participant while they were completing the scenario and if we saw them struggling
                        or having confusion we noted this down and asked them about it during the post interview
                        questionnaire.</p>
                    <p>E.g... We noticed you had trouble looking through the different timetables, could you tell us why
                        you had so much trouble?
                        By using the answers to these questions we will be able to perform some analysis of the
                        frequency of their answers and what was a reoccuring theme. This is discussed further in the <a
                            href="analysis.html">analysis section.</a></p>
                    <p>3: Before we used this scenario and interview above, we wanted to ensure the instructions were
                        clear and participants would be able to run through it with little issues. We did this by
                        conducting a pilot scenario where we got one participant to run through everything and ensure
                        that the instructions were understood. (We didn’t record or count this as a participant).
                        From this pilot we were able to see if 5 minutes was enough time, if the instructions were clear
                        and if the questionnaire would provide enough feedback to create some suggestions for the
                        developers. The conclusion of this pilot showed the team that we need to tweak the wording of
                        the scenario to be more clear. Apart from this, we found that 5 minutes was more than enough
                        time and the questions we asked at the end were informative enough to provide suggestions to the
                        developers.
                    </p>
                    <p>4: Once the team was happy with the scenario and the interview questions, we set up a computer in
                        the R&D lab in WZ701 where we conducted all interviews and scenarios using the same computer in
                        the same environment. We also ensured that all sessions were recorded using screen recording
                        software so that we could go back and see how the participants were interacting with the
                        application. By doing this, we were able to look back if we needed any clarification and to
                        cross-reference against their interview answers. This was also useful as we could see exactly
                        where participants were getting stuck and where they were spending the majority of their time.
                    </p>
                    <h3>Environment Setup</h3>
                    <p>We wanted to use the same environment for all participants during the study for general
                        consistency and best practice. This consists of the following constraints.</p>
                    <li>We used the same computer in the Research and Development lab in WZ701.</li>
                    <li>We used Chrome as the browser which will be used to access the application.</li>
                    <li>The same chair, mouse, keyboard and screen were used throughout the testing.</li>
                    <li>We conducted all testing within the same time frame (between 2 - 5PM on the same day)</li>
                    <h3>Execution</h3>
                    <p>Once we had the environment and tests set up, we needed to start asking people if they wanted to
                        be apart of the study. We used convenience sampling to find these participants, which consisted
                        of people from the lab we were conducting the study in.</p>
                    <p>From here, we took them over to the machine we were using, explained to them what the basic idea
                        of the application was and asked them to read the scenario. We also told all the participants to
                        openly talk about what they were thinking while they were using the software, but they were not
                        required to talk if they didn’t want to. We also mentioned that they can ask us questions
                        throughout but we won't be able to answer anything until the end.</p>
                    <p>Once they were ready and knew what their task was, we opened us the screen recording software
                        (Windows Game Bar) and let them run through the scenario unguided.</p>
                    <p>After the completion of the task or 5 minutes had elapsed, we asked them to stop and we
                        immediately conducted the post interview. If they had any questions while using the application
                        we wrote them down and asked them about their confusion during this time.</p>


                    <hr>
                </div>

                <footer>
                    <p class="references">
                        <strong>References</strong>
                    </p>
                    <p class="referencesList">
                        1. Barnum, C. M. (2010). Usability Testing Essentials: Ready, Set…Test! Amsterdam: Elsevier.<br>
                        2. Budiu, R. (2017). Quantitative vs. Qualitative Usability Testing. Retrieved from https://www.nngroup.com/articles/quant-vs-qual/<br>
                        3. Dumas, J. S., & Redish, J. C. (1999). A Practical Guide to Usability Testing. Exeter, England: Intellect.<br>
                        4. Gleason, D. (2019). Qualitative vs. quantitative user research: the answers you will (and won’t) get from each. Retrieved from https://www.hotjar.com/blog/qualitative-vs-quantitative-user-research/<br>
                        5. Seffah, A., & Metzker, E. (2009). Adoption-centric usability engineering: systematic deployment, assessment, and improvement of usability methods in software engineering. London: Springer.
                    </p>
                </footer>
</body>

</html>