<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <link href="https://fonts.googleapis.com/css?family=Montserrat&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
    <script src="https://kit.fontawesome.com/d61fe607bc.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="style/style.css">
    <title>AHCI</title>
</head>

<body>
    <header>
        <!-- Navbar below -->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary">
            <a class="navbar-brand" href="index.html"><i class="fas fa-laptop-code"></i>AHCI</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarText"
                aria-controls="navbarText" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarText">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item ">
                        <a class="nav-link" href="index.html">Home</a>
                    </li>
                    <li class="nav-item ">
                        <a class="nav-link" href="methodology.html">Methodology</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="analysis.html">Analysis</a>
                    </li>
                    <li class="nav-item px--1">
                        <a class="nav-link" href="results.html">Results</a>
                    </li>
                    <li class="nav-item dropdown px-2">
                        <a class="nav-link dropdown-toggle" href="#" id="navbardrop" data-toggle="dropdown">
                          Reflections
                        </a>
                        <div class="dropdown-menu">
                          <a class="dropdown-item" href="yash.html">Yash Chaudhari</a>
                          <a class="dropdown-item" href="rachel.html">Rachel Lee</a>
                          <a class="dropdown-item" href="matt.html">Matt Ripia</a>
                          <a class="dropdown-item" href="jamie.html">Jaime King</a>
                        </div>
                      </li>
                </ul>
            </div>
        </nav>
    </header>

    <div class="body">
        <div class="jumbotron feature">
            <div class="container">
                <h1><strong>Jaime King</strong></h1>
            </div>
        </div>

        <!-- Content -->
        <div class="container">

            <!-- Page Intro -->
            <div class="row page-intro">
                <div class="introduction_paragraph">
                    <h2>Personal Reflection</h2>

                    <p>For my reflection, I would like to cover the following topics; choosing the 
                        right questions, creating a good scenario, and usability test iteration. 
                    I will also touch on my time at AUT and my industry experience.</p>

                    <p>For this usability study, we asked a series of questions to participants after 
                        they had used the application. In hindsight I have realised that some of the 
                        questions we asked were very obvious and the results show this. For example, 
                        we asked “If you had another go would it be easier for you now that you have 
                        used it once?” this question received the same answer from every participant, 
                        “yes”.  I would assume that for any task someone would improve if they tried 
                        to do it again, unless the task was completely confusing and they had no idea 
                        what they had even done. There are a couple other questions like this and 
                        the issue with them is that they take extra time which can leave a participant 
                        frustrated and cause them to limit their other answers. They also interfere 
                        with the analysis of the data as that data is almost worthless. In the future 
                        it would be a good idea to have the questions reviewed by someone more experienced 
                        that will pick up on these flaws. </p>

                    <p>Secondly, I would like to talk about the scenario that we used to test the 
                        application. I wrote the scenario with input from the rest of the team. While 
                        I think the scenario was clear and well written there are some comments to be 
                        made. After we first wrote the scenario we got a ‘guinea pig’ to come and test 
                        our test for us. This revealed a flaw in the scenario, the original wording 
                        was “You’re about to plan your timetable for the second half of 2020.” Our 
                        guinea pig got confused as to what semester they should choose so as a result 
                        we changed the wording to “You’re about to plan your timetable for 2020 Semester 
                        2.” We did this in an effort to make the scenario more clear. We also highlighted 
                        the critical constraints in bold so that the participants could refer back to 
                        them quicker during the test. This appeared to work as there were no more 
                        participants that had the same issue.</p>

                    <p>Finally, I would like to talk about iteration. From conducting this study, I 
                        have realised the benefits of performing regular usability tests on software. 
                        They can identify issues that people already familiar to the application would 
                        overlook. If we were to perform another usability study on the same application 
                        with the suggested improvements it would be interesting to see what results we 
                        find. Whether the changes we suggested are effective or not.</p>

                    <p>I have been studying the Bachelor of Computer and Information Sciences course at 
                        AUT for three years now, my major is software development. Last summer I had 
                        the opportunity to take part in a n internship with the NZ Transport Agency 
                        as a developer, this internship gave me the opportunity to apply the skills 
                        that I have learnt throughout the course. At the time, I had only completed 
                        two years of the course so I was still missing some knowledge and this meant 
                        I had a steep learning curve to catch up with the technologies that are used 
                        in industry. In hindsight, I wish that I applied myself more to the internship 
                        role and made better efforts to upskill myself so that I could run with the 
                        bulls so to speak. </p>

                    <p>To conclude, this assessment provided a good insight into software usability 
                        testing and I will hopefully be able to apply this new knowledge in the 
                        industry once I start as a software developer.</p>

                    <hr>
                </div>
                <footer>
                        <p class="references">
                                <strong>References</strong>
                            </p>
                            <p class="referencesList">
                                1. Barnum, C. M. (2010). Usability Testing Essentials: Ready, Set…Test! Amsterdam: Elsevier.<br>
                                2. Budiu, R. (2017). Quantitative vs. Qualitative Usability Testing. Retrieved from https://www.nngroup.com/articles/quant-vs-qual/<br>
                                3. Dumas, J. S., & Redish, J. C. (1999). A Practical Guide to Usability Testing. Exeter, England: Intellect.<br>
                                4. Gleason, D. (2019). Qualitative vs. quantitative user research: the answers you will (and won’t) get from each. Retrieved from https://www.hotjar.com/blog/qualitative-vs-quantitative-user-research/<br>
                                5. Seffah, A., & Metzker, E. (2009). Adoption-centric usability engineering: systematic deployment, assessment, and improvement of usability methods in software engineering. London: Springer.
                            </p>
                </footer>
</body>

</html>